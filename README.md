# ğŸ“šğŸ¤– RAG + LLM: Document Ingestion, Embeddings & Contextual Generation

---

## ğŸ” Overview
This repo demonstrates an end-to-end **Retrieval-Augmented Generation (RAG)** pipeline:  
PDF ğŸ“„ ingestion â†’ text chunking âœ‚ï¸ â†’ embeddings ğŸ”— â†’ ChromaDB ğŸ“¦ â†’ retriever ğŸ” â†’ LLM ğŸ¤– â†’ contextual answers with sources.  
Built using **LangChain**, **HuggingFace**, **Groq LLMs**, and **Chroma**. ğŸš€  

---

## âš™ï¸ Setup

### Prerequisites
- Python 3.8+ ğŸ  
- pip  
- (Optional) GPU for local inference  
- `poppler` installed for `unstructured[pdf]` parsing  

### Install Poppler
- Ubuntu/Debian: `sudo apt-get install -y poppler-utils`  
- macOS: `brew install poppler`  

### Create Environment
```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -U chromadb langchain langchain-groq langchain-community \
    langchain-chroma langchain-text-splitters transformers \
    sentence-transformers unstructured "unstructured[pdf]"
```
---

## ğŸ§© Components
- **Document Loader** â€” `UnstructuredFileLoader`  
- **Text Splitter** â€” `CharacterTextSplitter` (`chunk_size=1000`, `chunk_overlap=50`)  
- **Embeddings** â€” `HuggingFaceEmbeddings`  
- **Vector DB** â€” `Chroma` (`persist_directory="vector_db"`)  
- **Retriever** â€” `vectordb.as_retriever()`  
- **LLM** â€” `ChatGroq` (`llama-3.3-70b-versatile`)  
- **QA Chain** â€” `RetrievalQA.from_chain_type(...)`  
- **Utilities** â€” `requests` for fetching example PDFs  

---

## âœ¨ Features
- End-to-end workflow (ingest â†’ retrieve â†’ generate)  
- Persistent vector DB for re-use âš¡  
- HuggingFace embeddings for local inference ğŸ§   
- Real example dataset (IPL 2025 schedule) ğŸ  
- RetrievalQA with source citations ğŸ“‘  

---

## ğŸ“ Responsibilities
- **UnstructuredFileLoader** â†’ Parse PDFs into Document objects  
- **CharacterTextSplitter** â†’ Break docs into manageable chunks  
- **HuggingFaceEmbeddings** â†’ Convert text chunks â†’ vectors  
- **Chroma** â†’ Store & index vectors  
- **Retriever** â†’ Fetch nearest neighbors for queries  
- **ChatGroq (LLM)** â†’ Generate context-aware responses  
- **RetrievalQA** â†’ Connect retriever + LLM + return answers + sources  

---

## ğŸ› ï¸ Technologies Used
- Python 3.8+ ğŸ  
- LangChain ğŸ”—  
- `langchain-groq` (Groq LLM wrapper)  
- `langchain-chroma` & `chromadb`  
- `langchain-community`  
- Hugging Face sentence-transformers ğŸ¤—  
- `transformers`  
- `unstructured[pdf]` ğŸ“„  
- Poppler (system dependency)  
- Requests ğŸŒ  

---

## ğŸ—ï¸ How it works (Architecture)

flowchart LR

  A[ğŸ“„ PDF / Documents] --> B[ğŸ“¥ UnstructuredFileLoader]
  
  B --> C[âœ‚ï¸ Text Splitter<br/>(chunk_size=1000,<br/>overlap=50)]
  
  C --> D[ğŸ”— Embeddings<br/>(HuggingFaceEmbeddings)]
  
  D --> E[ğŸ“¦ Chroma Vector DB<br/>(persist_directory="vector_db")]
  
  E --> F[ğŸ” Retriever (as_retriever())]
  
  F --> G[âš™ï¸ RetrievalQA Chain]
  
  H[ğŸ¤– LLM: ChatGroq<br/>(llama-3.3-70b-versatile)] --> G
  
  G --> I[âœ… Answer + ğŸ“‘ Source Documents]
  

- Flow summary: Ingest â†’ Chunk â†’ Embed â†’ Index â†’ Retrieve â†’ Generate Answer
---
## ğŸš€ Quick Example (Usage)

```python
# ğŸ“¥ Download example PDF (IPL 2025 Schedule)
import requests
url = "https://documents.iplt20.com/smart-images/..._Season_Schedule_2025.pdf"
resp = requests.get(url)
open("IPL_Schedule_2025.pdf", "wb").write(resp.content)

# ğŸ“„ Load document
from langchain_community.document_loaders import UnstructuredFileLoader
loader = UnstructuredFileLoader("IPL_Schedule_2025.pdf")
documents = loader.load()

# âœ‚ï¸ Split into chunks
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
texts = text_splitter.split_documents(documents)

# ğŸ”— Generate embeddings + store in Chroma DB
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
embeddings = HuggingFaceEmbeddings()
vectordb = Chroma.from_documents(texts, embeddings, persist_directory="vector_db")

# ğŸ” Retriever + âš™ï¸ QA Chain
retriever = vectordb.as_retriever()
from langchain_groq import ChatGroq
llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)

from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# â“ Query the system
res = qa_chain({'query': 'Total runs scored by Virat Kohli'})
print(res['result'])
```
---
## ğŸ’¡ Use Cases
- ğŸ“„ Ad-hoc Q&A over PDFs & reports  
- ğŸ‘©â€ğŸ’» Lightweight support knowledge-bases  
- ğŸ“‘ Research assistants w/ citations  
- ğŸ¢ Enterprise documentation search  
- ğŸ§ª Prototyping RAG before scaling to cloud vector DBs  

---

## ğŸ“Š Summary Table

| Component     | Library / Class           | Notes                                  |
|---------------|---------------------------|----------------------------------------|
| Loader        | `UnstructuredFileLoader`  | Parses PDFs â†’ Document objects          |
| Splitter      | `CharacterTextSplitter`   | `chunk_size=1000`, `chunk_overlap=50` |
| Embeddings    | `HuggingFaceEmbeddings`   | Uses sentence-transformers             |
| Vector DB     | `Chroma`                  | Persistent DB at `vector_db/`          |
| Retriever     | `vectordb.as_retriever()` | KNN-based retrieval                    |
| LLM           | `ChatGroq`                | Model = `llama-3.3-70b-versatile`      |
| Orchestration | `RetrievalQA (LangChain)` | Returns answers + source documents     |

---

## ğŸ“š Learning Insights
- âœ‚ï¸ **Chunking matters**: balance between precision & recall (500â€“1500 tokens).  
- ğŸ”— **Embeddings**: choose domain-suited models for accuracy.  
- ğŸ’¾ **Persist vectors**: saves time on re-runs.  
- âš¡ **Cost vs latency**: cloud vs local embeddings & LLM trade-offs.  
- ğŸ”‘ **Never commit secrets**: always use env vars.  
- ğŸ“‘ **Return sources**: improves trustworthiness.  
- âœ… **Evaluate retrieval quality**: test retrieval with metrics + manual checks.  

---

## ğŸ™Œ Acknowledgements
- LangChain team & contributors ğŸ’¡  
- Groq & `langchain-groq` âš¡  
- ChromaDB / `langchain-chroma` ğŸ“¦  
- Hugging Face ğŸ¤—  
- `unstructured` project ğŸ“„  
- Example data: IPL 2025 public schedule ğŸ  

---

## ğŸ™ Thank You
Thank you for exploring this project! ğŸš€  
Your feedback, ideas, and contributions are always welcome. ğŸŒŸ  
Happy building with **RAG + LLMs** ğŸ¤–ğŸ“š!  


